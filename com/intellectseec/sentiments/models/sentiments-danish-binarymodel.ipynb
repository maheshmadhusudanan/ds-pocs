{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import xml.etree.ElementTree as ET\n",
    "import utils\n",
    "from utils import *\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from translate import translator\n",
    "from langdetect import detect\n",
    "from random import randrange\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "import tqdm\n",
    "import dill\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read danish articles from mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# initializing Mongo DB Client for infomedia articles\n"
     ]
    }
   ],
   "source": [
    "## db variables\n",
    "host = \"mongodb://localhost:27017\"\n",
    "uid = \"\"\n",
    "pwd = \"\"\n",
    "client = None\n",
    "db = None\n",
    "sentiment_collection = None\n",
    "\n",
    "# connect to mongo\n",
    "#\n",
    "print(\"############# initializing Mongo DB Client for infomedia articles\")\n",
    "mclient = MongoClient(host)\n",
    "mdb = mclient.sentiments_db\n",
    "infomedia_collection = mdb.infomedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the training dataset from file , if it does not exists then read it from db and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/sentiments/data /home/ubuntu/nbs/sentiments/data/original /home/ubuntu/nbs/sentiments/data/train\n",
      "model-ln-da-s50000-v2017-19-15.h5\n"
     ]
    }
   ],
   "source": [
    "## Global scope variables\n",
    "GLOBAL_WORDS_LIST = []\n",
    "WORD_TO_IDX = {}\n",
    "IDX_TO_WORD = {}\n",
    "CURRENT_LANGUAGE = \"da\"\n",
    "current_dir = os.getcwd()\n",
    "DATA_DIR = current_dir + \"/sentiments/data\"\n",
    "ORIGINAL_DATA = DATA_DIR + \"/original\"\n",
    "TRAIN_DATA_DIR = DATA_DIR + \"/train\"\n",
    "TRAIN_DATA_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_train_data.p\"\n",
    "TRAIN_LABEL_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_train_labels.p\"\n",
    "VALIDATE_DATA_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_validate_data.p\"\n",
    "VALIDATE_LABEL_FILE = DATA_DIR + \"/\" +CURRENT_LANGUAGE+\"_validate_labels.p\"\n",
    "MODEL_PATH = current_dir + \"/sentiments/models/\"\n",
    "WORD_TO_IDX_DATA_FILE = DATA_DIR + \"/\"+CURRENT_LANGUAGE+\"_words_to_idx.p\"\n",
    "IDX_TO_WORD_DATA_FILE = DATA_DIR + \"/\"+CURRENT_LANGUAGE+\"_idx_to_words.p\"\n",
    "print DATA_DIR, ORIGINAL_DATA, TRAIN_DATA_DIR\n",
    "# maximum num of words the sentiment can process\n",
    "MAX_WORD_COUNT = 500\n",
    "vocabsize = 50000\n",
    "RESERVED_PROCESS_TERM_IDX = vocabsize\n",
    "DB_RECORD_EXTRACTION_LIMIT = 50000\n",
    "MODEL_NAME = \"model-ln-\"+CURRENT_LANGUAGE+\"-s\"+str(vocabsize)+\"-v2017-19-15\"+\".h5\"\n",
    "print MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps Training:\n",
    "1. Read the articles with process terms and its sentiments.\n",
    "     1.a) the output will be an array of dict items\n",
    "          [{'id':'4s23s32', 'process_terms':'bestbuy', text:'bestbuy is better than staples', 'sentiment':0.1},\n",
    "           {'id':'4s23s32', 'process_terms':'staples', text:'bestbuy is better than staples', 'sentiment':1}]\n",
    "     \n",
    "2. Detect text language and translate into english\n",
    "3. Convert all words into index array. \n",
    "4. Split data into training and validation set. \n",
    "5. Fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_training_data_from_db(start=0, limit_size=10):\n",
    "    results = []\n",
    "    total = mdb.infomedia.find({}).count()\n",
    "    print(\"******** Total records in db = \"+str(total))\n",
    "    cursor = mdb.infomedia.find({}).skip(0).limit(DB_RECORD_EXTRACTION_LIMIT)\n",
    "#     f = FloatProgress(min=0, max=total)\n",
    "#     display(f)\n",
    "    for doc in cursor:\n",
    "        # print(doc)\n",
    "#         f.value += 1\n",
    "        r = {'id': str(doc.get('ArticleId')),\n",
    "             'text': doc.get('BodyText').encode('utf-8').lower().strip(),\n",
    "             'process_term': [],\n",
    "             'sentiment':0.0\n",
    "             }\n",
    "#         print(r)\n",
    "        process_terms= doc.get('kw')\n",
    "        scores = doc.get('score')\n",
    "        if process_terms != None:\n",
    "            for index,item in enumerate(process_terms):\n",
    "                rcopy = r.copy()\n",
    "                for x in item:\n",
    "                    pitem = x.encode(\"utf-8\").lower()\n",
    "                    rcopy['process_term'].extend([y for y in pitem.split(',')])\n",
    "                rcopy['process_term'] = set(rcopy['process_term'])   \n",
    "                rcopy['sentiment'] = scores[index]\n",
    "                results.append(rcopy)\n",
    "    \n",
    "#     print(\"found db entries = \"+results.length())\n",
    "#     print results[:10]\n",
    "    print(\"******** Total records extracted from db = \"+str(len(results)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "#get_articles_from_db();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_ln_word_idx_list(target_language, src_text):\n",
    "    global GLOBAL_WORDS_LIST\n",
    "    text= src_text.lower()\n",
    "#     text = src_text.decode('utf-8')\n",
    "#     source_ln = 'en'\n",
    "#     try:\n",
    "#         source_ln = detect(text)\n",
    "# #         source_ln = 'da'\n",
    "#     except:\n",
    "#         print(\"Failed to detect lang for text = \"+text[:30])\n",
    "#         return\n",
    "        \n",
    "# #     print(\" detected language = \"+source_ln)\n",
    "#     if source_ln == target_language:\n",
    "    words = text.split()\n",
    "    GLOBAL_WORDS_LIST.extend(words)\n",
    "#         GLOBAL_WORDS_LIST.extend(x for x in words if x not in GLOBAL_WORDS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_words_to_idx(text, process_terms):\n",
    "    global WORD_TO_IDX\n",
    "    word_idx = []\n",
    "    p_terms = [x.lower().replace('*','') for x in process_terms]\n",
    "    text = text.lower()\n",
    "    for word in text.split():\n",
    "        if word in p_terms:\n",
    "            word_idx.append(RESERVED_PROCESS_TERM_IDX)\n",
    "        else:\n",
    "            if (word not in WORD_TO_IDX):\n",
    "                word_idx.append(vocabsize - 1)\n",
    "#                 print (\"word not found in vocab = \"+word)\n",
    "            else:\n",
    "                word_idx.append(WORD_TO_IDX[word])\n",
    "    \n",
    "    return word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_words_index():\n",
    "    global GLOBAL_WORDS_LIST\n",
    "    global WORD_TO_IDX\n",
    "    global IDX_TO_WORD\n",
    "    ## \n",
    "    # Build the word to idx and idx to word and dump it for later use\n",
    "    #\n",
    "    print(\"***** total words found = \"+str(len(GLOBAL_WORDS_LIST)))\n",
    "    words_occurence = Counter(GLOBAL_WORDS_LIST)\n",
    "    print(\"***** total unique words = \"+str(len(words_occurence)))\n",
    "    sorted_words = [w for w,c in words_occurence.most_common(vocabsize-2)]\n",
    "    print (\"sorted word = \")\n",
    "    print(sorted_words[:10])\n",
    "    WORD_TO_IDX = {w:idx for idx,w in enumerate(sorted_words)}\n",
    "    IDX_TO_WORD = {idx:w for idx,w in enumerate(sorted_words)}\n",
    "    print (\"word to idx size = \"+str(len(WORD_TO_IDX)))        \n",
    "    print(\"id to word size = \"+str(len(IDX_TO_WORD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Total records in db = 59641\n",
      "******** Total records extracted from db = 65018\n",
      "***** total words found = 28615201\n",
      "***** total unique words = 661484\n",
      "sorted word = \n",
      "['i', 'og', 'at', 'er', 'det', 'en', 'til', 'p\\xc3\\x83\\xc2\\xa5', 'for', 'af']\n",
      "word to idx size = 49998\n",
      "id to word size = 49998\n",
      "*** converting texts to ids\n",
      "validation set size = 19510\n",
      "{'process_term': set(['semler: audi', 'audi*', 'semler holding a/s']), 'text': 'de tyske bilproducenter og is\\xc3\\x83\\xc2\\xa6r mercedes klarer sig godt i den nye tuv report 2017.  den store overraskelse er, at to nyere modeller fra kia, der giver syv \\xc3\\x83\\xc2\\xa5rs garanti p\\xc3\\x83\\xc2\\xa5 teknikken, klarer sig d\\xc3\\x83\\xc2\\xa5rligst i deres klasse. unders\\xc3\\x83\\xc2\\xb8gelse af cirka ni millioner i tyskland ligger til grund for rapporten.    fakta de bedste   de bedste indtil tre \\xc3\\x83\\xc2\\xa5r mercedes glk 2,1 porsche 911 carrera 2,1 mercedes b-klasse 2,2 mercedes a-klasse 2,3 mercedes slk 2,4 de bedste indtil fem \\xc3\\x83\\xc2\\xa5r mercedes slk 2,9 audi a6/ a7 4,2 auti tt 4,4 audi q5 4,8 porsche 911 carrera 4,8 de bedste indtil syv \\xc3\\x83\\xc2\\xa5r mazda3 6,8 porsche 911 carrera 7,4 audi tt 7,7 bmw x1 8 toyota avensis 8,1 de bedste indtil ni \\xc3\\x83\\xc2\\xa5r porsche 911 carrera 9,9 audi tt 11,5 mazda2 12,4 mercedes slk 12,8 toyota auris 12,9 de bedste indtil 11 \\xc3\\x83\\xc2\\xa5r: porsche 911 carrera 10,4 toyota corolla verso 15,8 mercedes slk 17,4 toyota rav4 17,8 honda cr-v 18 de d\\xc3\\x83\\xc2\\xa5rligste indtil tre \\xc3\\x83\\xc2\\xa5r: kia sportage 11,5 kia sorento 11,2 chevrolet captiva 11 chevrolet spark 10,5 fiat punto 10,5 de d\\xc3\\x83\\xc2\\xa5rligste indtil fem \\xc3\\x83\\xc2\\xa5r: dacia logan 22,6 renault kangoo 18,5.', 'id': 'e6302a1b', 'sentiment': 0}\n",
      "[11, 670, 12856, 1, 280, 1640, 1977, 34, 73, 0, 15, 46, 49999, 15838, 553, 15, 70, 6251, 160, 2, 58, 3377, 1964, 19, 49999, 10, 139, 410, 393, 2464, 7, 44651, 1977, 34, 10757, 0, 59, 11817, 956, 9, 392, 904, 155, 0, 1944, 159, 6, 246, 8, 11324, 561, 11, 262, 11, 262, 399, 119, 43, 1640, 49999, 4716, 2160, 5273, 19118, 4716, 1640, 49999, 2825, 1640, 49999, 3013, 1640, 38943, 5310, 11, 262, 399, 217, 43, 1640, 38943, 3274, 50000, 49999, 47667, 6760, 49999, 14314, 7624, 50000, 10500, 7903, 2160, 5273, 19118, 7903, 11, 262, 399, 410, 43, 49999, 13323, 2160, 5273, 19118, 15826, 50000, 14314, 12713, 1275, 49999, 1578, 786, 8082, 18162, 11, 262, 399, 904, 43, 2160, 5273, 19118, 37102, 50000, 14314, 10898, 49999, 11931, 1640, 38943, 13938, 786, 26122, 30023, 11, 262, 399, 749, 11250, 2160, 5273, 19118, 25815, 786, 22334, 49999, 49999, 1640, 38943, 49999, 786, 49999, 49431, 5701, 49999, 983, 11, 14257, 399, 119, 11250, 2439, 42254, 10898, 2439, 49999, 49999, 24991, 49999, 749, 24991, 13862, 12096, 3433, 49999, 12096, 11, 14257, 399, 217, 11250, 23898, 48526, 49999, 2017, 49999, 49999]\n",
      "0\n",
      "[277, 49999, 30988, 213, 287, 49999, 657, 287, 767, 49999, 3645, 220, 27449, 5571, 0, 6527, 5, 2706, 14, 4523, 252, 2815, 1522, 49999, 28, 4523, 657, 287, 37, 6108, 0, 34557, 49999, 30988, 66, 2248, 49999, 174, 49999, 287, 49999, 867, 5571, 0, 3194, 0, 14224, 13, 5077, 12, 35757, 49999, 36741, 657, 287, 37, 6108, 0, 2070, 49999, 14, 101, 657, 287, 3645, 220, 27449, 5571, 0, 49999, 49999, 49999]\n",
      "0\n",
      "validation data size = 19510\n",
      "validation labels size = 19510\n",
      "train data size = 45508\n",
      "train labels size = 45508\n",
      "***** saved data to pickle files..\n"
     ]
    }
   ],
   "source": [
    "def setup_training_data():\n",
    "    global GLOBAL_WORDS_LIST\n",
    "    global WORD_TO_IDX\n",
    "    global IDX_TO_WORD\n",
    "    results = load_training_data_from_db()\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    train_process_terms = [[]]\n",
    "    word_list = []\n",
    "    GLOBAL_WORDS_LIST = []\n",
    "    \n",
    "    f = FloatProgress(min=0, max=len(results))\n",
    "    display(f)\n",
    "    \n",
    "    ##\n",
    "    # Build a word list from all the training data\n",
    "    #\n",
    "    for rec in results:\n",
    "        f.value += 1\n",
    "        build_ln_word_idx_list(CURRENT_LANGUAGE, rec['text'].lower())\n",
    "    \n",
    "    #\n",
    "    # load the words index\n",
    "    #\n",
    "    load_words_index()\n",
    "    \n",
    "    #    \n",
    "    # convert the train_data words to indexes\n",
    "    #\n",
    "    print(\"*** converting texts to ids\")\n",
    "    f.value = 0\n",
    "    for rec in results:\n",
    "        f.value += 1\n",
    "        train_data.append(convert_words_to_idx(rec['text'].lower(),rec['process_term']))\n",
    "        train_labels.append(rec['sentiment'])\n",
    "    \n",
    "    #\n",
    "    # pick random items from list for validation set\n",
    "    #\n",
    "    validate_data = []\n",
    "    validate_labels = []\n",
    "    val_set_size = int(round((len(train_data) * 0.3), -1))\n",
    "    print(\"validation set size = \"+str(val_set_size))\n",
    "    for i in range(0,val_set_size):\n",
    "        random_index = randrange(i,len(train_data))\n",
    "        validate_data.append(train_data[random_index])\n",
    "        validate_labels.append(train_labels[random_index])\n",
    "        del train_data[random_index]\n",
    "        del train_labels[random_index]\n",
    "       \n",
    "    print(results[1])\n",
    "    print(train_data[1])\n",
    "    print(train_labels[1])\n",
    "    print(validate_data[1])\n",
    "    print(validate_labels[1])\n",
    "    print(\"validation data size = \"+str(len(validate_data)))\n",
    "    print(\"validation labels size = \"+str(len(validate_labels)))\n",
    "    print(\"train data size = \"+str(len(train_data)))\n",
    "    print(\"train labels size = \"+str(len(train_labels)))\n",
    "    #\n",
    "    # save it for later use. \n",
    "    #\n",
    "    dill.dump(GLOBAL_WORDS_LIST, open(WORD_TO_IDX_DATA_FILE, \"wb\" ))\n",
    "    print(\"***** saved GLOBAL_WORDS_LIST to pickle files..\")\n",
    "    dill.dump(train_data, open(TRAIN_DATA_FILE, \"wb\" ))\n",
    "    print(\"***** saved train_data to pickle files..\")\n",
    "    dill.dump(train_labels, open(TRAIN_LABEL_FILE, \"wb\" ))\n",
    "    print(\"***** saved train_labels to pickle files..\")\n",
    "    dill.dump(validate_data, open(VALIDATE_DATA_FILE, \"wb\" ))\n",
    "    print(\"***** saved validate_data to pickle files..\")\n",
    "    dill.dump(validate_labels, open(VALIDATE_LABEL_FILE, \"wb\" ))\n",
    "    print(\"***** saved validate_labels to pickle files..\")\n",
    "    \n",
    "    print(\"***** SAVED ALL FILES.....\")\n",
    "   \n",
    "\n",
    "\n",
    "setup_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_training_data_from_file():\n",
    "    global GLOBAL_WORDS_LIST\n",
    "    global WORD_TO_IDX\n",
    "    global IDX_TO_WORD\n",
    "    GLOBAL_WORDS_LIST = []\n",
    "    \n",
    "    WORD_TO_IDX = {}\n",
    "    IDX_TO_WORD = {}\n",
    "    GLOBAL_WORDS_LIST = pickle.load(open( WORD_TO_IDX_DATA_FILE, \"rb\" ))\n",
    "    \n",
    "    #\n",
    "    # load training data from file\n",
    "    #\n",
    "    train_data = pickle.load(open(TRAIN_DATA_FILE, \"rb\" ))\n",
    "    train_labels = pickle.load(open(TRAIN_LABEL_FILE, \"rb\" ))\n",
    "    validate_data = pickle.load(open(VALIDATE_DATA_FILE, \"rb\" ))\n",
    "    validate_labels = pickle.load(open(VALIDATE_LABEL_FILE, \"rb\" ))\n",
    "     #\n",
    "    # load the words index\n",
    "    #\n",
    "    load_words_index()\n",
    "    \n",
    "    return train_data,train_labels, validate_data, validate_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code\n",
    "1. load the traing and validation data from disk\n",
    "2. Pad the data to match the shape of the Cnv\n",
    "3. Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_padded_data(word_idx):        \n",
    "    return sequence.pad_sequences(word_idx,maxlen=MAX_WORD_COUNT, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** total words found = 28615201\n",
      "***** total unique words = 661484\n",
      "sorted word = \n",
      "['i', 'og', 'at', 'er', 'det', 'en', 'til', 'p\\xc3\\x83\\xc2\\xa5', 'for', 'af']\n",
      "word to idx size = 49998\n",
      "id to word size = 49998\n",
      "===================================================================================\n",
      "1054\n",
      "konkurrence\n",
      "49998\n",
      "===================================================================================\n",
      "[[80, 21, 18, 156, 1203, 33, 13351, 3, 793, 43779, 8481, 0, 15, 396, 7823, 22, 496, 56, 565, 181, 76, 2, 7969, 741, 5, 118, 981, 0, 49999, 4, 354, 417, 389, 109, 9, 11, 2482, 8859, 0, 98, 49999, 46, 49999, 75, 11, 3911, 2800, 0, 1605, 3, 4, 87, 461, 848, 10, 691, 2, 59, 7969, 12, 1020, 8, 33122, 1450, 3, 83, 6473, 1, 3839, 0, 49999, 8238, 49999, 16, 895, 131, 2, 236, 56, 9, 2438, 3717, 2, 11, 12, 60, 1919, 8, 5, 49999, 0, 49999, 82, 1284, 474, 2, 5, 2366, 2185, 7, 1191, 20, 540, 0, 20604, 9, 528, 1, 2854, 41, 11, 3568, 49999, 26, 528, 1, 565, 8921, 6934, 0, 49999, 41, 528, 4958, 21, 5, 443, 4024, 42, 11, 11587, 14, 49999, 33, 565, 42, 11587, 14, 49999, 4, 29, 281, 2482, 6230, 1142, 1, 5374, 4, 3, 49999, 1, 49999, 49999, 10, 741, 1436, 3385, 49999, 1476, 0, 49999, 36, 32016, 3, 3392, 40, 11, 46, 6729, 32, 1547, 2, 2541, 17, 87, 243, 21, 20604, 9, 46359, 0, 49999, 23, 114, 22, 869, 2, 528, 1, 565, 102, 86, 6, 3499, 7, 232, 49999, 26, 1511, 438, 2, 528, 121, 49999, 14, 5, 17512, 49999, 33, 565, 3, 39, 49999, 565, 121, 344, 49999, 14, 16, 30345, 4311, 1, 3873, 34, 40, 27216, 6, 4355, 1, 2793, 79, 49999, 8, 26299, 4, 3, 107, 17, 87, 16, 679, 21, 26809, 0, 49999, 14, 3, 7801, 121, 24, 7, 5083, 9, 49999, 68, 3911, 1, 2482, 13070, 22, 12, 2457, 29, 656, 5780, 125, 0, 125, 1650, 565, 49999, 89, 21, 1904, 26, 38, 314, 33902, 89, 21, 416, 26, 171, 55, 59, 3911, 49999, 1450, 3, 1196, 7, 2457, 111, 5, 790, 1, 7095, 33, 10, 3, 273, 2349, 8, 4637, 0, 38222, 3699, 49999, 7727, 1, 49999, 561, 21, 49999, 389, 109, 9, 2438, 0, 1605, 2721, 2, 59, 7969, 2907, 59, 36342, 75, 11, 3911, 2800, 0, 1605, 3, 4, 87, 461, 481, 121, 24, 76, 7, 130, 24, 11906, 34, 6, 49999, 11906, 2438, 0, 1605, 34, 552, 39, 55, 11, 3911, 16106, 39, 55, 210, 51, 9, 581, 9, 11, 2482, 7661, 283, 2, 11, 17, 145, 14, 10599, 36, 93, 49999, 4, 1406, 371, 8, 11, 3911, 2800, 0, 1605, 28, 5, 51, 9, 13541, 1010, 98, 49999, 49999, 553]]\n",
      "-------------------------------\n",
      "[1]\n",
      "-------------------------------\n",
      "validation data size = 19510\n",
      "validation labels size = 19510\n",
      "train data size = 45508\n",
      "train labels size = 45508\n"
     ]
    }
   ],
   "source": [
    "t_data, t_labels, v_data, v_labels = load_training_data_from_file()\n",
    "print(\"===================================================================================\")\n",
    "print WORD_TO_IDX[u'konkurrence']\n",
    "print IDX_TO_WORD[WORD_TO_IDX[u'konkurrence']]\n",
    "print len(WORD_TO_IDX)\n",
    "index = randrange(0,len(t_data))\n",
    "print(\"===================================================================================\")\n",
    "print(t_data[index:index+1])\n",
    "print(\"-------------------------------\")\n",
    "print(t_labels[index:index+1])\n",
    "print(\"-------------------------------\")\n",
    "print(\"validation data size = \"+str(len(v_data)))\n",
    "print(\"validation labels size = \"+str(len(v_labels)))\n",
    "print(\"train data size = \"+str(len(t_data)))\n",
    "print(\"train labels size = \"+str(len(t_labels)))\n",
    "\n",
    "\n",
    "\n",
    "# print(pt[index:index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45508\n",
      "19510\n",
      "(45508, 500)\n",
      "(19510, 500)\n",
      "[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Counter({1: 8514, 0: 8028, -1: 1977, 0.5: 606, -0.5: 385})\n",
      "50000\n",
      "500\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0    11   670 12856     1   280  1640  1977    34    73\n",
      "     0    15    46 49999 15838   553    15    70  6251   160     2    58  3377  1964    19 49999\n",
      "    10   139   410   393  2464     7 44651  1977    34 10757     0    59 11817   956     9   392\n",
      "   904   155     0  1944   159     6   246     8 11324   561    11   262    11   262   399   119\n",
      "    43  1640 49999  4716  2160  5273 19118  4716  1640 49999  2825  1640 49999  3013  1640 38943\n",
      "  5310    11   262   399   217    43  1640 38943  3274 50000 49999 47667  6760 49999 14314  7624\n",
      " 50000 10500  7903  2160  5273 19118  7903    11   262   399   410    43 49999 13323  2160  5273\n",
      " 19118 15826 50000 14314 12713  1275 49999  1578   786  8082 18162    11   262   399   904    43\n",
      "  2160  5273 19118 37102 50000 14314 10898 49999 11931  1640 38943 13938   786 26122 30023    11\n",
      "   262   399   749 11250  2160  5273 19118 25815   786 22334 49999 49999  1640 38943 49999   786\n",
      " 49999 49431  5701 49999   983    11 14257   399   119 11250  2439 42254 10898  2439 49999 49999\n",
      " 24991 49999   749 24991 13862 12096  3433 49999 12096    11 14257   399   217 11250 23898 48526\n",
      " 49999  2017 49999 49999]\n"
     ]
    }
   ],
   "source": [
    "pad_train_data = get_padded_data(t_data)\n",
    "pad_validation_data = get_padded_data(v_data)\n",
    "print len(pad_train_data)\n",
    "print len(pad_validation_data)\n",
    "print pad_train_data.shape\n",
    "print pad_validation_data.shape\n",
    "v_bin_labels = [1 if x>=0 else 0 for x in v_labels]\n",
    "t_bin_labels = [1 if x>=0 else 0 for x in t_labels]\n",
    "print v_bin_labels[:20]\n",
    "print t_bin_labels[:20]\n",
    "print(Counter(v_labels))\n",
    "print vocabsize\n",
    "print MAX_WORD_COUNT\n",
    "print pad_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 500, 32)       1600032     embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 500, 32)       0           embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_4 (Convolution1D)  (None, 500, 64)       10304       dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 500, 64)       0           convolution1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_4 (MaxPooling1D)    (None, 250, 64)       0           dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 16000)         0           maxpooling1d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 100)           1600100     flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 100)           0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             101         dropout_12[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 3210537\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabsize+1, output_dim=32, input_length=MAX_WORD_COUNT, dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution1D(64, 5, activation='relu', border_mode='same'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(),  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45508 samples, validate on 19510 samples\n",
      "Epoch 1/5\n",
      "45508/45508 [==============================] - 25s - loss: 0.2977 - acc: 0.8735 - val_loss: 0.2369 - val_acc: 0.9133\n",
      "Epoch 2/5\n",
      "45508/45508 [==============================] - 24s - loss: 0.2075 - acc: 0.9117 - val_loss: 0.2063 - val_acc: 0.9313\n",
      "Epoch 3/5\n",
      "45508/45508 [==============================] - 25s - loss: 0.1684 - acc: 0.9334 - val_loss: 0.1952 - val_acc: 0.9301\n",
      "Epoch 4/5\n",
      "45508/45508 [==============================] - 24s - loss: 0.1468 - acc: 0.9409 - val_loss: 0.2016 - val_acc: 0.9158\n",
      "Epoch 5/5\n",
      "45508/45508 [==============================] - 24s - loss: 0.1311 - acc: 0.9464 - val_loss: 0.2154 - val_acc: 0.9175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d466e91d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pad_train_data,t_bin_labels,validation_data=(pad_validation_data, v_bin_labels), nb_epoch=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(MODEL_PATH + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test / Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(MODEL_PATH + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text=\"Der er opstået en fejl: Beskeden er sendt. * ) skal udfyldes.  Næsten hver anden firmabil er en Mercedes, BMW eller Audi. Andelen voksede i fjor, og tendensen ser ud til at fortsætte. De danske firmabilister er vilde med jobbiler, der har fire ringe, brede nyrer eller en stjerne på snuden, og andelen vokser fortsat. Mens Audi, BMW og Mercedes i 2015 med 5479 stk. stk. udgjorde 44,6 pct. i den vigtige klasse af erhvervsleasede firmabiler i den såkaldte D-størrelse, var andelen i fjor med 6461 stk. på 45,7 pct.  Især Mercedes, der har skruet ned for priserne og op for markedsføringskanonerne, og har lanceret nye modeller som E-klassen, er gået frem. Og hvis vi tæller modeller som Volvo S60/V60 og VW Passat med, der ligger på kanten af at kunne kalde sig premium, er der ikke meget tilbage til de andre mærker.  Der er flere årsager til fremgangen for premiummærkerne Audi, Mercedes og BMW, vurderer branchefolk, vi har talt med. De tror, at tendensen vil fortsætte i 2017. Mads Iversen,der er operations director hos Leaseplan, peger på den hårde konkurrence:  - Audi, Mercedes og BMW har alle kørt med attraktive kampagner, og de er presset af Volvo. Og så er det alle biler med en høj restværdi, så de samlede udgifter, den såkaldte TCO, er lav, siger han.  Thomas Lindgren Mortensen, som er adm. direktør for Alm. Brand Leasing, pointerer, at det i store træk ikke er dyrere for firmaet at vælge Mercedes frem for en Ford Mondeo, fordi den samlede TCO er den samme. Brugeren skal så vurdere, om vedkommende vil beskattes højere. Og det vælger stadig flere, især i takt med, at premiummærkerne har sænket de beskatningsmæssige værdier, så forskellen ikke er så stor.  F.eks. kan du hos Nordania Leasing få en Ford Mondeo stationcar med Titaniumudstyr, 180 hk dieselmotor og automatgear til en beskatningsværdi på 368.000 kr. mens en BMW 3-serie stationcar med 190 hk dieselmotor og automatgear samt businessudstyrspakke ligger på 404.000 kr. Hvis du kan nøjes med manuelt gear, er beskatningsværdien 374.000 kr.  Den nye Mercedes E-klasse starter fra en beskatningsmæssig værdi på 520.000 kr. for en 220 D sedan Business 9gtronic med automatgear og 194 hk hos Nordania Leasing. Her følger den nye BMW 5-serie trop med beskatningsværdier fra 510.000 kr. for en 520d sedan aut. med 190 hk hos BMW Finans.  Samtidig har importørerne udvidet viften af muligheder. BMW markedsfører også 3-serie GT, 4-serien og 4-serie Coupé, som ellers tidligere har været nichemodeller. Det giver udslag, også hos Leaseplan.  - Vi har kunder, der er kommet ind efter en Ford Mondeo eller Opel Insignia, men er endt med at køre i BMW 3-serie, en Audi A4 eller en Mercedes C-klasse. Det er blevet mere legalt at køre BMW. Faren er selvfølgelig også, at det ikke er så eksklusivt som tidligere, men det er det op til bilfabrikkerne at vurdere konsekvensen af, siger Mads Iversen fra Leaseplan.  Hos Alm. Brand ser direktør Thomas Lindgren Mortensen også i stigende grad, at flere firmaer giver grønt lys til, at medarbejderne må køre i premiummærker - og i særdeleshed Mercedes:  - Tidligere så vi en del virksomheder, som havde en politik om, at man ikke måtte køre i Mercedes. Men det er ved at ændre sig mange steder, konstaterer ThomasLindgren Mortensen, som tror, at viften bliver bredere, så et mærke som Jaguar vil få mere fat.  Her har det engelske mærke på det seneste skruet op for markedsføringen af både XE, XF, og den nye SUV, F-Pace, som er kommet godt fra land:  - Det har været med Jaguar som med Mercedes tidligere. Men i dag er der ikke noget ekstravagant i at køre Jaguar, så det mærke vil vi se mere til, mener Thomas Lindgren Mortensen.  Men også nedsættelse af bilafgiften i henholdsvis 2015 og 2016, hvor de beskatningsmæssige værdier faldt med henholdsvis omkring en snes og en halv snes tusinde kr., mens brugerne stadig har den samme ramme at købe bil for, har medført et større rådighedsbeløb og dermed ændringer. Thomas Lindgren Mortensen kan se, at afgiftsnedsættelserne har rykket ved købsmønstret:  - Vi ser, at folk bruger de ekstra penge til enten at købe ekstraudstyr eller købe en dyrere bil, fordi afstanden er blevet mindre. De vælger ikke at spare pengene og måske gå en bilklasse ned. Så det er et godt argument for at sænke afgiften yderligere, siger Thomas Lindgren Mortensen, som tror, at de, som nu har valgt et premiummærke, holder fast.  - Når du først har vænnet dig til at køre en bil fra et premiummærke, er det svært at gå tilbage, siger han.  De nye muligheder med større rådighedsbeløb smitter af på flere fronter, lyder meldingen fra Mads Iversen fra Leaseplan.  - Firmabilisterne har fået videre rammer at købe bil for, og de penge bruger de især på sikkerhedsudstyr som f.eks. adaptiv fartpilot eller full LED-lys. Det ligger fint i tråd med virksomhedernes ønsker om mere sikre arbejdspladser og forøger samtidig bilernes gensalgsværdi. Premiummærkerne har været i stand til at sammensætte udstyrspakker med disse sikkerhedselementer til attraktive priser, hvilket f.eks. Opel og Ford ikke har haft samme muligheder for.  - Vi vil også hellere sælge en brugt bil med en stor sikkerhedspakke frem for f.eks. læderinteriør, store alufælge eller indbyggede dvd-skærme, siger Mads Iversen.  Han tror lige som Thomas Lindgren Mortensen, at udviklingen fortsætter i 2017:  - Premiummærkerne vil holde samme andel og måske få en lille stigning, hvis tendensen holder, og der ikke sker ændringer. Premiummærkerne holder øje med hinanden. Hvis Audi, Mercedes og BMW er skarpe på tilbud med især sikkerhedsudstyr som for eksempel adaptiv fartpilot, automatisk linjevogter og alarmer for blinde vinkler, vil udviklingen kunne fortsætte. Men de vil også blive presset af Volvo, som er på vej med en ny XC40, hvor prisen efter sigende skulle blive skarp, siger Mads Iversen, der også forudser en fortsat fremgang for crossover/suv-segmentet, hvor Nissan Qashqai har godt fat, og nyheder som bl.a. Peugeot 3008, Peugeot 5008 og Seat Altea nu for alvor gør deres indtog i sammen med VW Tiguan\"\n",
    "proc_terms=[\n",
    "    \"Econic, Smart, Unimog, Viano, Vito, Mercedes-Benz*, Mercedes*, Daimler\",\n",
    "        \"Econic, Smart, Unimog, Viano, Vito, Mercedes-Benz*, Mercedes*, Daimler\",\n",
    "        \"brand, alm brand*, Alm.brand, Alm. Brand\",\n",
    "        \"Audi*\",\n",
    "        \"jaguar\",\n",
    "        \"Jaguar, Land Rover, Range Rover, Landrover*\",\n",
    "        \"Jaguar, Land Rover, Range Rover, Landrover*\"\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "[[ 0.242]]\n"
     ]
    }
   ],
   "source": [
    "#text_clean = re.sub('\\W+', ' ', text)\n",
    "textWordsIdxs = convert_words_to_idx(text,proc_terms)\n",
    "textIdxArrayPadded = get_padded_data([np.array(textWordsIdxs)])\n",
    "prediction = model.predict(textIdxArrayPadded, batch_size=1,verbose=1)\n",
    "print prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "dd658d8147594328b3636ae82b7d2c9c": {
     "views": [
      {
       "cell_index": 10.0
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}